{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b75cdd-148f-4f7a-a550-428aef7d5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "'''\n",
    "hit_fingerprints = hit_fingerprints[..., None].astype(\"float32\")\n",
    "x_train = x_train[..., None].astype(\"float32\")\n",
    "y_train = y_train.astype(\"int\")\n",
    "'''\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a molecular fingerprint.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 10\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(2048, 1))\n",
    "x = layers.Conv1D(filters = 32, kernel_size = (3,), activation = \"relu\")(encoder_inputs)\n",
    "x = layers.MaxPool1D(pool_size = (2,))(x)\n",
    "x = layers.Conv1D(filters = 64, kernel_size = (3,), activation = \"relu\")(x)\n",
    "x = layers.MaxPool1D(pool_size = (2,))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation = 'relu')(x)\n",
    "x = layers.Dropout(rate = 0.3)(x)\n",
    "z_mean = layers.Dense(latent_dim, name = \"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(128, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(64 * (2048 // (2 ** 2)), activation=\"relu\")(x)\n",
    "x = layers.Reshape(((2048 // (2 ** 2)), 64))(x)\n",
    "x = layers.Conv1DTranspose(filters=64, kernel_size=(3,), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.UpSampling1D(size=2)(x)\n",
    "x = layers.Conv1DTranspose(filters=32, kernel_size=(3,), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.UpSampling1D(size=2)(x)\n",
    "decoder_outputs = layers.Conv1DTranspose(filters = 1, kernel_size = (3,), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if len(data.shape) == 2:  \n",
    "            data = ops.expand_dims(data, axis=-1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            reconstruction_loss = ops.mean(reconstruction_loss)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(hit_fingerprints, epochs=30, batch_size=128)\n",
    "\n",
    "\n",
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data, verbose=0)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
